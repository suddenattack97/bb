"""
TCN V2 ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ â€” Triple Barrier + Heikin-Ashi + ì•ŒíŒŒ í”¼ì²˜

ë³€ê²½ì :
1. íƒ€ê²Ÿ: Triple Barrier (ìµì ˆì„ /ì†ì ˆì„ /ì‹œê°„) â†’ 0=í•˜ë½, 1=íš¡ë³´, 2=ìƒìŠ¹
2. ì…ë ¥: Heikin-Ashi ìº”ë“¤ + ë³€ë™ì„± í”¼ì²˜
3. ì†ì‹¤: ë°©í–¥ì„± ê°€ì¤‘ CrossEntropy (upâ†”down í˜¼ë™ ì‹œ 5ë°° í˜ë„í‹°)
4. ì„ íƒ: OI, Funding, CVD (ë°ì´í„° ìˆìœ¼ë©´ ì¶”ê°€)

ì‹¤í–‰: python -u train_initial_v2.py  (ë˜ëŠ” python train_initial_v2.py)
ì¶œë ¥: tcn_v2_model.pth, scaler_v2.npy
"""
import sys
import numpy as np

# ì½˜ì†” ì‹¤ì‹œê°„ ì¶œë ¥
def _log(*a, **kw):
    kw.setdefault('flush', True)
    print(*a, **kw)
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

from model_v2 import TCNClassifier

try:
    import pandas as pd
    import pandas_ta as ta
except ImportError as e:
    raise SystemExit(f"pandas / pandas_ta í•„ìš”: pip install pandas pandas_ta â€” {e}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  Triple Barrier ë¼ë²¨ ìƒì„±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def triple_barrier_labels(
    df: "pd.DataFrame",
    tp_pct: float = 0.005,   # ìµì ˆ 0.5%
    sl_pct: float = 0.005,   # ì†ì ˆ 0.5%
    barrier_minutes: int = 5,
    progress_fn=None,       # progress_fn(ì™„ë£Œìˆ˜, ì „ì²´ìˆ˜, í¼ì„¼íŠ¸) í˜¸ì¶œ
) -> np.ndarray:
    """
    ê° í–‰(ì‹œì )ì—ì„œ barrier_minutes ë’¤ê¹Œì§€ì˜ OHLCë¥¼ ë³´ê³ 
    ìµì ˆì„ /ì†ì ˆì„ /ì‹œê°„ ì¤‘ ë¨¼ì € ë‹¿ì€ ê²ƒìœ¼ë¡œ ë¼ë²¨ ê²°ì •.
    ë°˜í™˜: 0=í•˜ë½(ì†ì ˆ), 1=íš¡ë³´, 2=ìƒìŠ¹(ìµì ˆ)
    """
    n = len(df)
    total = n - barrier_minutes
    step = max(1, total // 20)  # ì•½ 5%ë§ˆë‹¤ ì§„í–‰ë¥  ì¶œë ¥
    labels = np.full(n, 1, dtype=np.int64)  # ê¸°ë³¸ê°’ íš¡ë³´

    highs = df["high"].values
    lows = df["low"].values
    closes = df["close"].values
    opens = df["open"].values  # iloc ë£¨í”„ ì œê±°ìš©

    for i in range(total):
        entry = closes[i]
        upper = entry * (1 + tp_pct)
        lower = entry * (1 - sl_pct)

        hit_upper = False
        hit_lower = False

        for j in range(1, barrier_minutes + 1):
            h = highs[i + j]
            l = lows[i + j]
            o = opens[i + j]
            # ë™ì¼ ë´‰ì— ìƒÂ·í•˜ë‹¨ ë™ì‹œ í„°ì¹˜ ì‹œ: ì‹œê°€ ê¸°ì¤€ íŒë‹¨
            if h >= upper and l <= lower:
                if o >= (upper + lower) / 2:
                    hit_upper = True
                else:
                    hit_lower = True
                break
            if h >= upper:
                hit_upper = True
                break
            if l <= lower:
                hit_lower = True
                break

        if hit_upper:
            labels[i] = 2  # ìƒìŠ¹
        elif hit_lower:
            labels[i] = 0  # í•˜ë½
        else:
            labels[i] = 1  # íš¡ë³´

        if progress_fn and (i + 1) % step == 0:
            progress_fn(i + 1, total, 100 * (i + 1) / total)

    if progress_fn and total > 0 and total % step != 0:
        progress_fn(total, total, 100.0)
    return labels


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  Heikin-Ashi ë³€í™˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def heikin_ashi(df: "pd.DataFrame") -> "pd.DataFrame":
    """OHLC â†’ Heikin-Ashi OHLC"""
    ha_close = (df["open"] + df["high"] + df["low"] + df["close"]) / 4
    ha_open = pd.Series(index=df.index, dtype=float)
    ha_open.iloc[0] = (df["open"].iloc[0] + df["close"].iloc[0]) / 2
    for i in range(1, len(df)):
        ha_open.iloc[i] = (ha_open.iloc[i - 1] + ha_close.iloc[i - 1]) / 2
    ha_high = pd.concat([df["high"], ha_open, ha_close], axis=1).max(axis=1)
    ha_low = pd.concat([df["low"], ha_open, ha_close], axis=1).min(axis=1)
    return pd.DataFrame({
        "open": ha_open,
        "high": ha_high,
        "low": ha_low,
        "close": ha_close,
        "volume": df["volume"],
    }, index=df.index)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  ë°©í–¥ì„± ê°€ì¤‘ CrossEntropy
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class DirectionalLoss(nn.Module):
    """ë°©í–¥(upâ†”down)ì„ í‹€ë¦´ ë•Œ ë” í° í˜ë„í‹°"""

    def __init__(self, wrong_direction_weight: float = 5.0):
        super().__init__()
        self.wrong_direction_weight = wrong_direction_weight

    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        # targets: 0=down, 1=flat, 2=up
        n = logits.size(0)
        weights = torch.ones_like(targets, dtype=torch.float32, device=logits.device)
        pred_class = logits.argmax(dim=1)
        # up(2) vs down(0) í˜¼ë™ = ìµœëŒ€ í˜ë„í‹°
        wrong_dir = ((pred_class == 0) & (targets == 2)) | ((pred_class == 2) & (targets == 0))
        weights[wrong_dir] = self.wrong_direction_weight
        return F.cross_entropy(logits, targets, reduction="none") * weights


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  ë°ì´í„°ì…‹
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class TripleBarrierDataset(Dataset):
    def __init__(
        self,
        csv_file: str,
        seq_len: int = 60,
        barrier_minutes: int = 5,
        use_heikin_ashi: bool = True,
        use_oi_funding: bool = False,
        oi_csv: str = None,
        funding_csv: str = None,
    ):
        _log("V2 ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
        # 2022ë…„ ì´í›„ë§Œ ë©”ëª¨ë¦¬ì— ë¡œë“œ (ê³¼ê±° ë°ì´í„°ëŠ” ì‹œì¥ êµ¬ì¡°ê°€ ë‹¬ë¼ ì œì™¸)
        df = pd.read_csv(csv_file, index_col="timestamp", parse_dates=True)
        df = df[~df.index.duplicated(keep="first")].sort_index()
        before = len(df)
        df = df.loc["2022-01-01":]
        _log(f"  CSV ë¡œë“œ: {before:,}í–‰ â†’ 2022ë…„ ì´í›„ {len(df):,}í–‰ ì‚¬ìš©")

        orig_volume = df["volume"].copy()
        if use_heikin_ashi:
            _log("  Heikin-Ashi ë³€í™˜ ì¤‘...")
            ha = heikin_ashi(df.copy())
            df = ha.copy()
            df["volume"] = orig_volume
            _log("  Heikin-Ashi ì™„ë£Œ")

        # Heikin-Ashi ê¸°ì¤€ í”¼ì²˜
        _log("  í”¼ì²˜ ê³„ì‚° ì¤‘ (log_return, volatility, RSI...)")
        df["log_return"] = np.log(df["close"] / df["close"].shift(1)).fillna(0)
        df["volatility"] = np.log((df["high"] - df["low"] + 1e-8) / (df["close"] + 1e-8) + 1e-8).fillna(0)
        df["rsi_14"] = ta.rsi(df["close"], length=14).fillna(50)
        typical = (df["high"] + df["low"] + df["close"]) / 3
        cum_tpv = (typical * df["volume"]).cumsum()
        cum_vol = df["volume"].cumsum()
        df["vwap"] = pd.Series(np.where(cum_vol > 0, cum_tpv / cum_vol, typical), index=df.index).ffill().bfill()
        bb_mid = df["close"].rolling(20).mean()
        bb_std = df["close"].rolling(20).std().fillna(0.01)
        df["bb_width"] = ((bb_mid + 2 * bb_std) - (bb_mid - 2 * bb_std)) / df["close"].fillna(0.02)
        df["bb_position"] = ((df["close"] - (bb_mid - 2 * bb_std)) / (4 * bb_std + 1e-8)).clip(0, 2).fillna(0.5)
        prev_close = df["close"].shift(1).fillna(df["close"])
        tr = np.maximum(df["high"] - df["low"], np.maximum(np.abs(df["high"] - prev_close), np.abs(df["low"] - prev_close)))
        df["atr_14"] = (tr.rolling(14).mean() / df["close"]).ffill().bfill().fillna(0.005)

        # CVD ëŒ€ë¦¬: (close-open) * volume ì˜ ëˆ„ì  (ë°©í–¥ì„± ê±°ë˜ëŸ‰)
        df["directional_vol"] = (df["close"] - df["open"]) * df["volume"]
        df["cvd_proxy"] = df["directional_vol"].rolling(20).sum().fillna(0) / (df["volume"].rolling(20).sum() + 1e-8)

        # OI / Funding (íŒŒì¼ ìˆìœ¼ë©´ ë³‘í•©)
        if use_oi_funding and oi_csv:
            try:
                oi_df = pd.read_csv(oi_csv, index_col=0, parse_dates=True)
                oi_df = oi_df.reindex(df.index, method="ffill").fillna(0)
                df["oi_change"] = oi_df.iloc[:, 0].pct_change().fillna(0)
            except Exception:
                df["oi_change"] = 0.0
        else:
            df["oi_change"] = 0.0

        if use_oi_funding and funding_csv:
            try:
                fr_df = pd.read_csv(funding_csv, index_col=0, parse_dates=True)
                fr_df = fr_df.reindex(df.index, method="ffill").fillna(0)
                df["funding_rate"] = fr_df.iloc[:, 0]
            except Exception:
                df["funding_rate"] = 0.0
        else:
            df["funding_rate"] = 0.0

        df = df.dropna()
        _log("  í”¼ì²˜ ê³„ì‚° ì™„ë£Œ")

        # Triple Barrier ë¼ë²¨
        def _on_progress(done, total, pct):
            _log(f"  Triple Barrier ë¼ë²¨: {done:,} / {total:,} ({pct:.0f}%)")
        _log("  Triple Barrier ë¼ë²¨ ê³„ì‚° ì¤‘...")
        labels = triple_barrier_labels(df, barrier_minutes=barrier_minutes, progress_fn=_on_progress)
        df["label"] = labels

        self.seq_len = seq_len
        self.barrier_minutes = barrier_minutes

        features = [
            "log_return", "volatility", "rsi_14", "vwap", "volume",
            "bb_width", "bb_position", "atr_14", "cvd_proxy",
            "oi_change", "funding_rate",
        ]
        self.feature_names = [f for f in features if f in df.columns]
        raw = df[self.feature_names].values.astype(np.float32)

        self.mean = raw.mean(axis=0)
        self.std = raw.std(axis=0) + 1e-8
        self.data_norm = (raw - self.mean) / self.std
        self.labels = df["label"].values.astype(np.int64)

        valid_end = len(df) - seq_len - barrier_minutes
        self.valid_len = max(0, valid_end)
        _log(f"ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ: {self.valid_len} ìƒ˜í”Œ, í”¼ì²˜ {len(self.feature_names)}ê°œ")

    def __len__(self) -> int:
        return self.valid_len

    def __getitem__(self, idx: int):
        x = self.data_norm[idx : idx + self.seq_len]
        y = self.labels[idx + self.seq_len]
        return torch.tensor(x), torch.tensor(y, dtype=torch.long)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  í•™ìŠµ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def train_v2(
    csv_file: str = None,
    oi_csv: str = "BTC_futures_oi.csv",
    funding_csv: str = "BTC_funding_rate.csv",
    epochs: int = 5,
    barrier_minutes: int = 5,
):
    from pathlib import Path
    # í˜„ë¬¼ OHLCV: fetch_data_v2ëŠ” BTC_all_1m_v2.csv, fetch_dataëŠ” BTC_all_1m.csv
    if csv_file is None:
        csv_file = "BTC_all_1m_v2.csv" if Path("BTC_all_1m_v2.csv").exists() else "BTC_all_1m.csv"

    # OI/Funding íŒŒì¼ ìˆìœ¼ë©´ ìë™ ë³‘í•©
    use_oi = Path(oi_csv).exists()
    use_funding = Path(funding_csv).exists()
    if use_oi:
        _log(f"OI í”¼ì²˜ ì‚¬ìš©: {oi_csv}")
    if use_funding:
        _log(f"Funding í”¼ì²˜ ì‚¬ìš©: {funding_csv}")

    dataset = TripleBarrierDataset(
        csv_file,
        seq_len=60,
        barrier_minutes=barrier_minutes,
        use_heikin_ashi=True,
        use_oi_funding=(use_oi or use_funding),
        oi_csv=oi_csv if use_oi else None,
        funding_csv=funding_csv if use_funding else None,
    )
    num_features = len(dataset.feature_names)

    train_loader = DataLoader(
        dataset,
        batch_size=512,
        shuffle=True,
        num_workers=0,
        pin_memory=True,
    )

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = TCNClassifier(num_features=num_features, num_classes=3).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    criterion = DirectionalLoss(wrong_direction_weight=5.0)

    for epoch in range(epochs):
        model.train()
        total_loss = 0.0
        correct = 0
        total = 0
        for i, (x, y) in enumerate(train_loader):
            x, y = x.to(device), y.to(device)
            optimizer.zero_grad()
            logits = model(x)
            loss = criterion(logits, y).mean()
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
            correct += (logits.argmax(1) == y).sum().item()
            total += y.size(0)

            if i % 500 == 0:
                _log(f"  Epoch [{epoch+1}/{epochs}] Step [{i}/{len(train_loader)}] "
                      f"Loss: {loss.item():.4f} Acc: {100*correct/max(1,total):.2f}%")

        avg_loss = total_loss / len(train_loader)
        acc = 100 * correct / total
        _log(f"âœ… Epoch {epoch+1}/{epochs} ì™„ë£Œ â€” Loss: {avg_loss:.4f} Acc: {acc:.2f}%")

    torch.save(model.state_dict(), "tcn_v2_model.pth")
    np.save("scaler_v2.npy", {
        "mean": dataset.mean,
        "std": dataset.std,
        "num_features": num_features,
        "feature_names": dataset.feature_names,
        "use_oi_funding": use_oi or use_funding,  # ì˜ˆì¸¡ ì‹œ OI/Funding ì‚¬ìš© ì—¬ë¶€
    })
    _log("ğŸ‰ V2 ëª¨ë¸ ì €ì¥ ì™„ë£Œ: tcn_v2_model.pth, scaler_v2.npy")


if __name__ == "__main__":
    train_v2()
